---
sidebar: sidebar 
permalink: infra/ai-aipod-nv-architecture.html 
keywords: NetApp AI, AI, Artificial Intelligence, ML, Machine Learning, NVIDIA, NVIDIA AI Enterprise, NVIDIA BasePOD, NVIDIA DGX 
summary: NVIDIA DGX 시스템을 탑재한 NetApp AIPod - 아키텍처 
---
= NVIDIA DGX H100 시스템을 탑재한 NVA-1173 NetApp AIPod - 솔루션 아키텍처
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ../media/


[role="lead"]
이 섹션에서는 NVIDIA DGX 시스템을 탑재한 NetApp AIPod 의 아키텍처에 중점을 둡니다.



== DGX 시스템을 갖춘 NetApp AIPod

이 참조 아키텍처는 컴퓨팅 노드 간에 400Gb/s InfiniBand(IB) 연결을 통해 컴퓨팅 클러스터 상호 연결 및 스토리지 액세스를 위해 별도의 패브릭을 활용합니다.  아래 그림은 DGX H100 시스템이 포함된 NetApp AIPod 의 전체 솔루션 토폴로지를 보여줍니다.

_NetApp AIpod 솔루션 토폴로지_

image:aipod-nv-a90-topo.png["입력/출력 대화 상자 또는 서면 내용을 나타내는 그림"]



== 네트워크 디자인

이 구성에서 컴퓨팅 클러스터 패브릭은 높은 가용성을 위해 서로 연결된 한 쌍의 QM9700 400Gb/s IB 스위치를 사용합니다.  각 DGX H100 시스템은 8개의 연결을 사용하여 스위치에 연결되며, 짝수 포트는 한 스위치에 연결되고 홀수 포트는 다른 스위치에 연결됩니다.

스토리지 시스템 액세스, 대역 내 관리 및 클라이언트 액세스를 위해 SN4600 이더넷 스위치 한 쌍이 사용됩니다.  스위치는 스위치 간 링크로 연결되고 여러 VLAN으로 구성되어 다양한 트래픽 유형을 분리합니다.  특정 VLAN 간에 기본 L3 라우팅이 활성화되어 동일한 스위치에 있는 클라이언트와 스토리지 인터페이스 간, 그리고 고가용성을 위한 스위치 간 다중 경로가 가능합니다.  대규모 배포의 경우, 필요에 따라 스파인 스위치용 추가 스위치 쌍과 추가 리프를 추가하여 이더넷 네트워크를 리프-스파인 구성으로 확장할 수 있습니다.

컴퓨팅 상호 연결 및 고속 이더넷 네트워크 외에도 모든 물리적 장치는 대역 외 관리를 위해 하나 이상의 SN2201 이더넷 스위치에 연결됩니다.  를 참조하십시오link:ai-aipod-nv-deploy.html["배포 세부 정보"] 네트워크 구성에 대한 자세한 내용은 페이지를 참조하세요.



== DGX H100 시스템의 스토리지 액세스 개요

각 DGX H100 시스템에는 관리 및 스토리지 트래픽을 위한 듀얼 포트 ConnectX-7 어댑터 2개가 제공되며, 이 솔루션의 경우 각 카드의 두 포트가 동일한 스위치에 연결됩니다.  각 카드의 한 포트는 LACP MLAG 본드로 구성되고, 한 포트는 각 스위치에 연결되며, 대역 내 관리, 클라이언트 액세스, 사용자 수준 스토리지 액세스를 위한 VLAN이 이 본드에 호스팅됩니다.

각 카드의 다른 포트는 AFF A90 스토리지 시스템에 연결하는 데 사용되며, 작업 부하 요구 사항에 따라 여러 구성으로 사용할 수 있습니다.  NVIDIA Magnum IO GPUDirect 스토리지를 지원하기 위해 RDMA를 통한 NFS를 사용하는 구성의 경우, 포트는 별도의 VLAN에 있는 IP 주소와 함께 개별적으로 사용됩니다.  RDMA가 필요하지 않은 배포의 경우 스토리지 인터페이스를 LACP 본딩으로 구성하여 높은 가용성과 추가 대역폭을 제공할 수도 있습니다.  RDMA 사용 여부와 관계없이 클라이언트는 NFS v4.1 pNFS 및 세션 트렁킹을 사용하여 스토리지 시스템을 마운트하여 클러스터의 모든 스토리지 노드에 대한 병렬 액세스를 활성화할 수 있습니다.  를 참조하십시오link:ai-aipod-nv-deploy.html["배포 세부 정보"] 클라이언트 구성에 대한 자세한 내용은 페이지를 참조하세요.

DGX H100 시스템 연결에 대한 자세한 내용은 다음을 참조하세요.link:https://nvdam.widen.net/s/nfnjflmzlj/nvidia-dgx-basepod-reference-architecture["NVIDIA BasePOD 문서"] .



== 저장 시스템 설계

각 AFF A90 스토리지 시스템은 각 컨트롤러에서 6개의 200GbE 포트를 사용하여 연결됩니다.  각 컨트롤러의 4개 포트는 DGX 시스템의 워크로드 데이터 액세스에 사용되고, 각 컨트롤러의 2개 포트는 클러스터 관리 아티팩트 및 사용자 홈 디렉토리에 대한 관리 플레인 서버의 액세스를 지원하기 위해 LACP 인터페이스 그룹으로 구성됩니다.  스토리지 시스템의 모든 데이터 액세스는 NFS를 통해 제공되며, AI 워크로드 액세스에 전담된 스토리지 가상 머신(SVM)과 클러스터 관리 용도에 전담된 별도의 SVM이 있습니다.

관리 SVM에는 각 컨트롤러에 구성된 2포트 인터페이스 그룹에 호스팅되는 단일 LIF만 필요합니다.  다른 FlexGroup 볼륨은 클러스터 노드 이미지, 시스템 모니터링 기록 데이터, 최종 사용자 홈 디렉토리와 같은 클러스터 관리 아티팩트를 보관하기 위해 관리 SVM에 프로비저닝됩니다.  아래 그림은 저장 시스템의 논리적 구성을 보여줍니다.

_NetApp A90 스토리지 클러스터 논리적 구성_

image:aipod-nv-a90-logical.png["입력/출력 대화 상자 또는 서면 내용을 나타내는 그림"]



== 관리 플레인 서버

이 참조 아키텍처에는 관리 플레인용으로 사용되는 5개의 CPU 기반 서버도 포함되어 있습니다.  이 시스템 중 두 개는 클러스터 배포 및 관리를 위한 NVIDIA Base Command Manager의 헤드 노드로 사용됩니다.  나머지 세 시스템은 Slurm을 사용하여 작업 스케줄링을 수행하는 배포를 위한 Kubernetes 마스터 노드나 로그인 노드와 같은 추가 클러스터 서비스를 제공하는 데 사용됩니다.  Kubernetes를 활용한 배포에서는 NetApp Trident CSI 드라이버를 활용하여 AFF A900 스토리지 시스템에서 관리 및 AI 워크로드를 위한 영구 스토리지와 함께 자동화된 프로비저닝 및 데이터 서비스를 제공할 수 있습니다.

각 서버는 클러스터 배포 및 관리를 가능하게 하기 위해 IB 스위치와 이더넷 스위치에 물리적으로 연결되며, 앞서 설명한 대로 클러스터 관리 아티팩트를 저장하기 위해 관리 SVM을 통해 스토리지 시스템에 NFS 마운트로 구성됩니다.

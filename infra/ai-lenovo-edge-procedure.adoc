---
sidebar: sidebar 
permalink: infra/ai-lenovo-edge-procedure.html 
keywords: procedure, operating system, ubuntu, nvidia, docker, criteo, brats 
summary: 이 섹션에서는 이 솔루션의 유효성을 검증하는 데 사용된 테스트 절차를 설명합니다. 
---
= 테스트 절차
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ../media/


[role="lead"]
이 섹션에서는 이 솔루션의 유효성을 검증하는 데 사용된 테스트 절차를 설명합니다.



== 운영 체제 및 AI 추론 설정

AFF C190 의 경우 NVIDIA 드라이버와 NVIDIA GPU를 지원하는 Docker가 포함된 Ubuntu 18.04를 사용했으며 MLPerf를 사용했습니다. https://github.com/mlperf/inference_results_v0.7/tree/master/closed/Lenovo["암호"^] Lenovo가 MLPerf Inference v0.7에 제출한 내용의 일부로 제공됩니다.

EF280의 경우 NVIDIA 드라이버와 NVIDIA GPU 및 MLPerf를 지원하는 Docker가 포함된 Ubuntu 20.04를 사용했습니다. https://github.com/mlcommons/inference_results_v1.1/tree/main/closed/Lenovo["암호"^] Lenovo가 MLPerf Inference v1.1에 제출한 내용의 일부로 제공됩니다.

AI 추론을 설정하려면 다음 단계를 따르세요.

. 등록이 필요한 데이터세트, ImageNet 2012 검증 세트, Criteo 테라바이트 데이터세트, BraTS 2019 훈련 세트를 다운로드한 다음 파일의 압축을 풉니다.
. 최소 1TB의 작업 디렉토리를 생성하고 환경 변수를 정의합니다. `MLPERF_SCRATCH_PATH` 디렉토리를 참조합니다.
+
네트워크 스토리지 사용 사례의 경우 공유 스토리지에서 이 디렉토리를 공유해야 하고, 로컬 데이터로 테스트하는 경우 로컬 디스크에서 공유해야 합니다.

. 메이크를 실행하다 `prebuild` 필요한 추론 작업을 위해 Docker 컨테이너를 빌드하고 시작하는 명령입니다.
+

NOTE: 다음 명령은 모두 실행 중인 Docker 컨테이너 내에서 실행됩니다.

+
** MLPerf 추론 작업을 위해 사전 학습된 AI 모델을 다운로드하세요. `make download_model`
** 무료로 다운로드할 수 있는 추가 데이터 세트를 다운로드하세요. `make download_data`
** 데이터 전처리: 만들기 `preprocess_data`
** 달리다: `make build` .
** 컴퓨팅 서버에서 GPU에 최적화된 추론 엔진을 구축하세요. `make generate_engines`
** 추론 워크로드를 실행하려면 다음 명령 하나를 실행하세요.




....
make run_harness RUN_ARGS="--benchmarks=<BENCHMARKS> --scenarios=<SCENARIOS>"
....


== AI 추론 실행

세 가지 유형의 실행이 실행되었습니다.

* 로컬 스토리지를 사용한 단일 서버 AI 추론
* 네트워크 스토리지를 활용한 단일 서버 AI 추론
* 네트워크 스토리지를 활용한 멀티 서버 AI 추론


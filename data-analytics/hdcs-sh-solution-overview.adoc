---
sidebar: sidebar 
permalink: data-analytics/hdcs-sh-solution-overview.html 
keywords: tr-4657, tr4657, 4657, hybrid cloud, spark, hadoop, aff, fas 
summary: 이 문서에서는 NetApp AFF 및 FAS 스토리지 시스템, NetApp Cloud Volumes ONTAP, NetApp 연결 스토리지, Spark 및 Hadoop용 NetApp FlexClone 기술을 사용하는 하이브리드 클라우드 데이터 솔루션을 설명합니다.  이러한 솔루션 아키텍처를 통해 고객은 자사 환경에 적합한 데이터 보호 솔루션을 선택할 수 있습니다.  NetApp 고객과의 상호 작용과 비즈니스 사용 사례를 기반으로 이러한 솔루션을 설계했습니다. 
---
= TR-4657: NetApp 하이브리드 클라우드 데이터 솔루션 - 고객 사용 사례 기반 Spark 및 Hadoop
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ../media/


Karthikeyan Nagalingam 및 Sathish Thyagarajan, NetApp

[role="lead"]
이 문서에서는 NetApp AFF 및 FAS 스토리지 시스템, NetApp Cloud Volumes ONTAP, NetApp 연결 스토리지, Spark 및 Hadoop용 NetApp FlexClone 기술을 사용하는 하이브리드 클라우드 데이터 솔루션을 설명합니다.  이러한 솔루션 아키텍처를 통해 고객은 자사 환경에 적합한 데이터 보호 솔루션을 선택할 수 있습니다.  NetApp 고객과의 상호 작용과 비즈니스 사용 사례를 기반으로 이러한 솔루션을 설계했습니다.  이 문서에서는 다음과 같은 자세한 정보를 제공합니다.

* Spark 및 Hadoop 환경에서 데이터 보호가 필요한 이유와 고객 과제
* NetApp 비전과 구성 요소 및 서비스를 기반으로 하는 데이터 패브릭입니다.
* 이러한 구성 요소를 사용하여 유연한 데이터 보호 워크플로를 구축하는 방법
* 실제 고객 사용 사례를 기반으로 한 여러 아키텍처의 장단점.  각 사용 사례는 다음 구성 요소를 제공합니다.
+
** 고객 시나리오
** 요구 사항 및 과제
** 솔루션
** 해결책 요약






== Hadoop 데이터 보호가 필요한 이유는?

Hadoop 및 Spark 환경에서는 다음과 같은 문제를 해결해야 합니다.

* *소프트웨어 또는 사람의 실패.*  Hadoop 데이터 작업을 수행하는 동안 소프트웨어 업데이트에서 인적 오류가 발생하면 작업에서 예상치 못한 결과가 발생할 수 있는 잘못된 동작이 발생할 수 있습니다.  이런 경우에는 실패나 부당한 결과를 방지하기 위해 데이터를 보호해야 합니다.  예를 들어, 교통 신호 분석 애플리케이션에 대한 소프트웨어 업데이트가 제대로 실행되지 않아 일반 텍스트 형태의 교통 신호 데이터를 제대로 분석하지 못하는 새로운 기능이 생겼습니다.  해당 소프트웨어는 여전히 JSON 및 기타 텍스트가 아닌 파일 형식을 분석하여 실시간 교통 제어 분석 시스템에서 데이터 포인트가 누락된 예측 결과를 생성합니다.  이런 상황은 신호등에서 사고로 이어질 수 있는 잘못된 출력을 유발할 수 있습니다.  데이터 보호는 이전에 작동하던 애플리케이션 버전으로 빠르게 롤백하는 기능을 제공함으로써 이 문제를 해결할 수 있습니다.
* *크기와 규모.*  데이터 소스와 볼륨이 끊임없이 증가함에 따라 분석 데이터의 크기도 날이 갈수록 커지고 있습니다.  소셜 미디어, 모바일 앱, 데이터 분석, 클라우드 컴퓨팅 플랫폼은 현재 빅데이터 시장의 주요 데이터 소스이며, 매우 빠르게 증가하고 있습니다. 따라서 정확한 데이터 작업을 보장하기 위해 데이터를 보호하는 것이 중요합니다.
* *Hadoop의 기본 데이터 보호.*  Hadoop에는 데이터를 보호하는 기본 명령이 있지만, 이 명령은 백업하는 동안 데이터의 일관성을 제공하지 않습니다.  디렉토리 수준의 백업만 지원합니다.  Hadoop에서 생성된 스냅샷은 읽기 전용이므로 백업 데이터를 직접 재사용하는 데 사용할 수 없습니다.




== Hadoop 및 Spark 고객을 위한 데이터 보호 과제

Hadoop 및 Spark 고객이 겪는 일반적인 과제는 데이터 보호 중에 프로덕션 클러스터의 성능에 부정적인 영향을 미치지 않으면서 백업 시간을 줄이고 백업 안정성을 높이는 것입니다.

또한 고객은 최적의 비즈니스 연속성을 위해 복구 지점 목표(RPO)와 복구 시간 목표(RTO) 가동 중지 시간을 최소화하고 온프레미스 및 클라우드 기반 재해 복구 사이트를 제어해야 합니다.  이러한 통제는 일반적으로 기업 수준의 관리 도구를 갖는 데서 비롯됩니다.

Hadoop과 Spark 환경은 데이터 볼륨이 엄청나고 계속 증가하고 있을 뿐만 아니라, 데이터가 도착하는 속도도 빨라지고 있기 때문에 복잡합니다.  이러한 시나리오에서는 소스 데이터로부터 효율적이고 최신 DevTest 및 QA 환경을 신속하게 만드는 것이 어렵습니다.  NetApp 이러한 과제를 인식하고 이 논문에서 제시하는 솔루션을 제공합니다.
